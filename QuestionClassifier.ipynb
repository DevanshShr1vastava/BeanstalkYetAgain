{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate correctness rate\n",
    "\n",
    "The Correcness rate represents the proportion of correct answers given by a user over their total attempts.\n",
    "\n",
    "We calculate it by \n",
    " - For each user, maintain a list of all their answers (correct or incorrect)\n",
    " - For each attempt, caculate the correctness rate as the ratio of correct answers to the total number of attempts up to that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correctness_rate(user_attempts):\n",
    "    correct_answers = sum(user_attempts)\n",
    "    total_attempts = len(user_attempts)\n",
    "    return correct_answers / total_attempts if total_attempts>0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate confidence level\n",
    "The confidence level is a mesaure of how confident a user might be in their answer, based on their performance and behavior (eg. marking the questions for review)\n",
    "\n",
    "We calculate it by \n",
    " - starting with a base confidence level of 3\n",
    " - for correct answer increase confidence +1 if first attempt, another +1 if question is not marked for review (max 5)\n",
    " - for incorrect answer decrease confidence -1 if the answer is marked for review,  another -1 for additional attempt beyond first (min 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence_level(row):\n",
    "\n",
    "    if row['answer'] == 1:\n",
    "        if(row['time_taken']<60):\n",
    "            ttC = 1\n",
    "        else:\n",
    "            ttC = 0\n",
    "        return min(5,3 + (row['attempt_number'] < 3) + (not row['marked_for_review']) + ttC)\n",
    "    else:\n",
    "        if(row['time_taken']<60):\n",
    "            ttC = 0\n",
    "        else:\n",
    "            ttC = 1\n",
    "        return max(1,3 - (row['marked_for_review']) - row['attempt_number']-ttC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to determine the label based on the defined rules i.e\n",
    "- attempt_number\n",
    "- correctness_rate\n",
    "- marked_for_review\n",
    "- confidence_level\n",
    "- answer\n",
    "\n",
    "Repeat the same question (label = 1) if\n",
    " - user has attempted the question multiple times (high `attempt_number`)\n",
    " - correctness rate is low\n",
    " - user marked the question for review\n",
    " - confidence level is low\n",
    " - answer is incorrect\n",
    " i.e\n",
    " attempt number > 1 and correctness rate < 0.7\n",
    " marked for review == 1\n",
    " confidence level < 3\n",
    " answer incorrect i.e == 0\n",
    "\n",
    "else \n",
    "\n",
    "label = 0\n",
    "Move on from the question (label = 0) if\n",
    "- The user answered the question correctly on the first or second attempt\n",
    "- the correctness rate is high\n",
    "- the user did not mark the question for review \n",
    "- the confidence level is high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_label(row):\n",
    "    if(row['attempt_number'] > 2 and row['correctness_rate'] < 0.7) or row['confidence_level'] < 3 or row['answer'] == 0:\n",
    "        return 1 #repeat the same question\n",
    "    else:\n",
    "        return 0 #move on from the question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generating a synthetic dataset (dummy data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "QuestionBank = pd.read_csv('questionBank.csv')\n",
    "\n",
    "qb_dbms_df = QuestionBank[QuestionBank['domain']=='DBMS']\n",
    "qb_dsa_df = QuestionBank[QuestionBank['domain']=='DSA']\n",
    "\n",
    "domains = [\"DBMS\",\"Data Structures\"]\n",
    "subdomains_dbms = list(qb_dbms_df['subdomain'].unique())\n",
    "subdomains_ds = list(qb_dsa_df['subdomain'].unique())\n",
    "subdomains = subdomains_dbms + subdomains_ds\n",
    "\n",
    "#initialize an empty list to collect rows\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "#generate data for users and questions\n",
    "\n",
    "for user_id in range(1,9): # 5 users\n",
    "    user_attempts = []\n",
    "    for question_id in range(100,400): # 300 questions\n",
    "        domain = np.random.choice(domains)\n",
    "        subdomain = np.random.choice(subdomains_dbms if domain == \"DBMS\" else subdomains_ds)\n",
    "        for attempt_number in range(1,np.random.randint(2,5)): # 1 to 3 attempts for questions\n",
    "            time_taken = np.random.randint(30,120) #20 to 120 seconds\n",
    "            answer = np.random.randint(0,2) # 0 or 1 (incorrect or correct)\n",
    "            marked_for_review = np.random.randint(0,2) # 0 or 1 (unmarked or marked)\n",
    "\n",
    "            #update user attempts and calculate correctness rate\n",
    "\n",
    "            user_attempts.append(answer)\n",
    "            correctness_rate = calculate_correctness_rate(user_attempts)\n",
    "\n",
    "            #Calculate teh confidence level\n",
    "\n",
    "            confidence_level = calculate_confidence_level({\n",
    "                'answer':answer,\n",
    "                'attempt_number':attempt_number,\n",
    "                'marked_for_review':marked_for_review,\n",
    "                'time_taken':time_taken\n",
    "            })\n",
    "\n",
    "            # Determine the label\n",
    "\n",
    "            label = determine_label({\n",
    "                'attempt_number' : attempt_number,\n",
    "                'correctness_rate':correctness_rate,\n",
    "                'marked_for_review':marked_for_review,\n",
    "                'confidence_level' : confidence_level,\n",
    "                'answer' : answer\n",
    "            })\n",
    "\n",
    "            #append the row to the Dataframe\n",
    "\n",
    "            row ={\n",
    "                'user_id':user_id,\n",
    "                'question_id':question_id,\n",
    "                'domain':domain,\n",
    "                'subdomain':subdomain,\n",
    "                'attempt_number':attempt_number,\n",
    "                'time_taken':time_taken,\n",
    "                'answer':answer,\n",
    "                'marked_for_review':marked_for_review,\n",
    "                'correctness_rate':correctness_rate,\n",
    "                'confidence_level':confidence_level,\n",
    "                'label':label\n",
    "            }\n",
    "            data.append(row)\n",
    "\n",
    "#check the dataset now\n",
    "df = pd.DataFrame(data)\n",
    "# print(df.head())\n",
    "\n",
    "df.to_csv('dummyData.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now\n",
    "Weighted KNN\n",
    "\n",
    "We need to prepare the data (train test split)\n",
    "train the model\n",
    "evaluate the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the sklearn stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset and split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the weighted KNN model, train the model and make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalutate the model now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next one seems to work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label for the new input: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('dummyData.csv')\n",
    "\n",
    "# Prepare the feature set and labels\n",
    "X = df.drop(columns=['label', 'user_id', 'question_id', 'domain', 'subdomain'])\n",
    "y = df['label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=34)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the weighted KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# New input data\n",
    "new_data = pd.DataFrame({\n",
    "    'attempt_number': [1],\n",
    "    'time_taken': [85],\n",
    "    'answer': [1],\n",
    "    'marked_for_review': [1],\n",
    "    'correctness_rate': [0.7],\n",
    "    'confidence_level': [3]\n",
    "})\n",
    "\n",
    "# Scale the new data using the same scaler\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "# Predict using the model\n",
    "prediction = knn.predict(new_data_scaled)\n",
    "\n",
    "# Print the result\n",
    "print(f'Predicted label for the new input: {prediction[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_category(row):\n",
    "    # Extract the percentage of repeats\n",
    "    percent_repeat = row['percent_repeat']\n",
    "    \n",
    "    # Define thresholds for categorization\n",
    "    if percent_repeat > 60:\n",
    "        return 2\n",
    "    elif percent_repeat < 50:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            subdomain      mean  count  percent_repeat  category\n",
      "0           Algorithm  0.518325    191       51.832461         1\n",
      "1          Algorithms  0.573394    218       57.339450         1\n",
      "2               Array  0.511450    131       51.145038         1\n",
      "3   Backup & Recovery  0.547170    159       54.716981         1\n",
      "4     Database Design  0.497175    177       49.717514         0\n",
      "5     Database Models  0.642857    168       64.285714         2\n",
      "6               Graph  0.579545    176       57.954545         1\n",
      "7             Hashing  0.594340    212       59.433962         1\n",
      "8                Heap  0.588745    231       58.874459         1\n",
      "9            Indexing  0.547619    168       54.761905         1\n",
      "10              Joins  0.574194    155       57.419355         1\n",
      "11               Keys  0.591623    191       59.162304         1\n",
      "12        Linked List  0.560870    230       56.086957         1\n",
      "13      Normalization  0.560847    189       56.084656         1\n",
      "14              Queue  0.570552    163       57.055215         1\n",
      "15         SQL Basics  0.527607    163       52.760736         1\n",
      "16    SQL Constraints  0.613095    168       61.309524         2\n",
      "17      SQL Functions  0.642045    176       64.204545         2\n",
      "18          Searching  0.598086    209       59.808612         1\n",
      "19           Security  0.643678    174       64.367816         2\n",
      "20            Sorting  0.610619    226       61.061947         2\n",
      "21              Stack  0.596059    203       59.605911         1\n",
      "22  Stored Procedures  0.570048    207       57.004831         1\n",
      "23       Transactions  0.611111    162       61.111111         2\n",
      "24               Tree  0.642157    204       64.215686         2\n",
      "25              Views  0.574194    155       57.419355         1\n",
      "            subdomain  category\n",
      "0           Algorithm         1\n",
      "1          Algorithms         1\n",
      "2               Array         1\n",
      "3   Backup & Recovery         1\n",
      "4     Database Design         0\n",
      "5     Database Models         2\n",
      "6               Graph         1\n",
      "7             Hashing         1\n",
      "8                Heap         1\n",
      "9            Indexing         1\n",
      "10              Joins         1\n",
      "11               Keys         1\n",
      "12        Linked List         1\n",
      "13      Normalization         1\n",
      "14              Queue         1\n",
      "15         SQL Basics         1\n",
      "16    SQL Constraints         2\n",
      "17      SQL Functions         2\n",
      "18          Searching         1\n",
      "19           Security         2\n",
      "20            Sorting         2\n",
      "21              Stack         1\n",
      "22  Stored Procedures         1\n",
      "23       Transactions         2\n",
      "24               Tree         2\n",
      "25              Views         1\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dummyData.csv')\n",
    "\n",
    "# Select the specified columns\n",
    "selected_columns = ['user_id', 'question_id', 'domain', 'subdomain', 'label']\n",
    "df_selected = df[selected_columns]\n",
    "\n",
    "# Print the selected DataFrame\n",
    "# print(df_selected)\n",
    "\n",
    "# Calculate percentages for each subdomain\n",
    "subdomain_stats = df_selected.groupby('subdomain')['label'].agg(['mean', 'count']).reset_index()\n",
    "subdomain_stats['percent_repeat'] = subdomain_stats['mean'] * 100\n",
    "\n",
    "subdomain_stats['category'] = subdomain_stats.apply(assign_category, axis=1)\n",
    "\n",
    "print(subdomain_stats)\n",
    "\n",
    "# The new dataset\n",
    "classified_data = subdomain_stats[['subdomain', 'category']]\n",
    "\n",
    "print(classified_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            subdomain  category  category_encoded\n",
      "0           Algorithm         1                 1\n",
      "1          Algorithms         1                 1\n",
      "2               Array         1                 1\n",
      "3   Backup & Recovery         1                 1\n",
      "4     Database Design         0                 0\n",
      "5     Database Models         2                 2\n",
      "6               Graph         1                 1\n",
      "7             Hashing         1                 1\n",
      "8                Heap         1                 1\n",
      "9            Indexing         1                 1\n",
      "10              Joins         1                 1\n",
      "11               Keys         1                 1\n",
      "12        Linked List         1                 1\n",
      "13      Normalization         1                 1\n",
      "14              Queue         1                 1\n",
      "15         SQL Basics         1                 1\n",
      "16    SQL Constraints         2                 2\n",
      "17      SQL Functions         2                 2\n",
      "18          Searching         1                 1\n",
      "19           Security         2                 2\n",
      "20            Sorting         2                 2\n",
      "21              Stack         1                 1\n",
      "22  Stored Procedures         1                 1\n",
      "23       Transactions         2                 2\n",
      "24               Tree         2                 2\n",
      "25              Views         1                 1\n",
      "\n",
      "Weighted KNN Accuracy: 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devansh\\AppData\\Local\\Temp\\ipykernel_15372\\2932094078.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  classified_data.loc[:,'category_encoded'] = label_encoder.fit_transform(classified_data['category'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Encoding the categories\n",
    "label_encoder = LabelEncoder()\n",
    "classified_data.loc[:,'category_encoded'] = label_encoder.fit_transform(classified_data['category'])\n",
    "print (classified_data)\n",
    "# Features and labels\n",
    "X = subdomain_stats[['percent_repeat']]\n",
    "y = classified_data['category_encoded']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=47)\n",
    "\n",
    "# Train Weighted KNN\n",
    "knn_model = KNeighborsClassifier(weights='distance')\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the models\n",
    "\n",
    "knn_accuracy = knn_model.score(X_test, y_test)\n",
    "\n",
    "\n",
    "print(f'\\nWeighted KNN Accuracy: {knn_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATING A NEW QUESTION PAPER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First creating a realistic data set on which we will run the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            subdomain  mean  count  percent_repeat\n",
      "0           Algorithm  0.25      4            25.0\n",
      "1          Algorithms  0.50      4            50.0\n",
      "2               Array  0.50      4            50.0\n",
      "3   Backup & Recovery  0.40      5            40.0\n",
      "4     Database Design  0.40      5            40.0\n",
      "5     Database Models  0.60      5            60.0\n",
      "6               Graph  0.25      4            25.0\n",
      "7             Hashing  0.50      4            50.0\n",
      "8                Heap  0.75      4            75.0\n",
      "9            Indexing  0.00      5             0.0\n",
      "10              Joins  0.20      5            20.0\n",
      "11               Keys  0.60      5            60.0\n",
      "12        Linked List  0.25      4            25.0\n",
      "13      Normalization  0.40      5            40.0\n",
      "14              Queue  0.50      4            50.0\n",
      "15         SQL Basics  0.00      5             0.0\n",
      "16    SQL Constraints  0.40      5            40.0\n",
      "17      SQL Functions  0.40      5            40.0\n",
      "18          Searching  0.80      5            80.0\n",
      "19           Security  0.60      5            60.0\n",
      "20            Sorting  1.00      4           100.0\n",
      "21              Stack  0.00      5             0.0\n",
      "22  Stored Procedures  0.60      5            60.0\n",
      "23       Transactions  0.20      5            20.0\n",
      "24               Tree  0.25      4            25.0\n",
      "25              Views  0.80      5            80.0\n",
      "            subdomain                               category  category_encoded\n",
      "0           Algorithm  Repeat Same Topic with Less Weightage                 1\n",
      "1          Algorithms  Repeat Same Topic with Less Weightage                 1\n",
      "2               Array  Repeat Same Topic with Less Weightage                 1\n",
      "3   Backup & Recovery  Repeat Same Topic with Less Weightage                 1\n",
      "4     Database Design  Repeat Same Topic with Less Weightage                 1\n",
      "5     Database Models  Repeat Same Topic with Less Weightage                 1\n",
      "6               Graph  Repeat Same Topic with Less Weightage                 1\n",
      "7             Hashing  Repeat Same Topic with Less Weightage                 1\n",
      "8                Heap  Repeat Same Topic with More Weightage                 2\n",
      "9            Indexing  Repeat Same Topic with Less Weightage                 1\n",
      "10              Joins  Repeat Same Topic with Less Weightage                 1\n",
      "11               Keys  Repeat Same Topic with Less Weightage                 1\n",
      "12        Linked List  Repeat Same Topic with Less Weightage                 1\n",
      "13      Normalization  Repeat Same Topic with Less Weightage                 1\n",
      "14              Queue  Repeat Same Topic with Less Weightage                 1\n",
      "15         SQL Basics  Repeat Same Topic with Less Weightage                 1\n",
      "16    SQL Constraints  Repeat Same Topic with Less Weightage                 1\n",
      "17      SQL Functions  Repeat Same Topic with Less Weightage                 1\n",
      "18          Searching  Repeat Same Topic with More Weightage                 2\n",
      "19           Security  Repeat Same Topic with Less Weightage                 1\n",
      "20            Sorting  Repeat Same Topic with More Weightage                 2\n",
      "21              Stack  Repeat Same Topic with Less Weightage                 1\n",
      "22  Stored Procedures  Repeat Same Topic with Less Weightage                 1\n",
      "23       Transactions  Repeat Same Topic with Less Weightage                 1\n",
      "24               Tree  Repeat Same Topic with Less Weightage                 1\n",
      "25              Views  Repeat Same Topic with More Weightage                 2\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    [1, 122, 'DBMS', 'SQL Basics', [1,1,1,1,1], 40, 1, 1],\n",
    "    [1, 133, 'DBMS', 'Normalization', [0, 0, 1, 0], 85, 0, 0],\n",
    "    [1, 144, 'DBMS', 'SQL Constraints', [1], 75, 1, 1],\n",
    "    [1, 155, 'DBMS', 'Keys', [1,1,1,1,1], 90, 0, 0],\n",
    "    [1, 166, 'DBMS', 'SQL Functions', [0, 0, 1, 0], 55, 1, 1],\n",
    "    [1, 177, 'DBMS', 'Joins', [1], 100, 1, 0],\n",
    "    [1, 188, 'DBMS', 'Stored Procedures', [1, 0, 1], 110, 0, 1],\n",
    "    [1, 199, 'DBMS', 'Transactions', [1,1,1,1,1], 65, 1, 0],\n",
    "    [1, 110, 'DBMS', 'Database Models', [1], 95, 0, 1],\n",
    "    [1, 121, 'DBMS', 'Indexing', [1,1,1,1,1], 70, 1, 0],\n",
    "    [1, 132, 'DBMS', 'Views', [1, 0, 1], 85, 0, 0],\n",
    "    [1, 143, 'DBMS', 'Database Design', [1], 100, 1, 1],\n",
    "    [1, 154, 'DBMS', 'Security', [0, 0, 1], 65, 0, 0],\n",
    "    [1, 165, 'DBMS', 'Backup & Recovery', [1], 105, 1, 1],\n",
    "    [1, 176, 'DSA', 'Searching', [0, 0, 1], 35, 0, 1],\n",
    "    [1, 187, 'DSA', 'Stack', [1], 115, 1, 0],\n",
    "    [1, 198, 'DSA', 'Sorting', [1, 0], 90, 0, 0],\n",
    "    [1, 109, 'DSA', 'Queue', [0, 0, 1], 50, 1, 1],\n",
    "    [1, 120, 'DSA', 'Graph', [1], 85, 0, 0],\n",
    "    [1, 131, 'DSA', 'Heap', [1,1,1], 60, 1, 0],\n",
    "    [1, 142, 'DSA', 'Tree', [1], 125, 0, 1],\n",
    "    [1, 153, 'DSA', 'Algorithms', [1, 0], 45, 1, 0],\n",
    "    [1, 164, 'DSA', 'Hashing', [1,1,1], 75, 0, 1],\n",
    "    [1, 175, 'DSA', 'Linked List', [1], 55, 1, 0],\n",
    "    [1, 186, 'DSA', 'Array', [1,1,1], 90, 0, 1],\n",
    "    [1, 197, 'DSA', 'Algorithm', [1, 0], 80, 1, 0],\n",
    "    [1, 108, 'DBMS', 'SQL Basics', [1, 0], 55, 1, 0],\n",
    "    [1, 119, 'DBMS', 'Normalization', [1], 70, 0, 1],\n",
    "    [1, 130, 'DBMS', 'SQL Constraints', [1,1,1], 60, 1, 0],\n",
    "    [1, 141, 'DBMS', 'Keys', [1, 0], 110, 0, 1],\n",
    "    [1, 152, 'DBMS', 'SQL Functions', [1], 50, 1, 0],\n",
    "    [1, 163, 'DBMS', 'Joins', [0, 0, 1], 95, 1, 0],\n",
    "    [1, 174, 'DBMS', 'Stored Procedures', [1], 80, 0, 1],\n",
    "    [1, 185, 'DBMS', 'Transactions', [1, 0], 120, 1, 0],\n",
    "    [1, 196, 'DBMS', 'Database Models', [0, 0, 1], 65, 0, 0],\n",
    "    [1, 107, 'DBMS', 'Indexing', [1], 105, 1, 1],\n",
    "    [1, 118, 'DBMS', 'Views', [1, 0], 70, 0, 0],\n",
    "    [1, 129, 'DBMS', 'Database Design', [0, 0, 1], 130, 1, 1],\n",
    "    [1, 140, 'DBMS', 'Security', [1], 60, 0, 0],\n",
    "    [1, 151, 'DBMS', 'Backup & Recovery', [1, 0], 100, 1, 1],\n",
    "    [1, 162, 'DSA', 'Searching', [0, 0, 1], 35, 0, 1],\n",
    "    [1, 173, 'DSA', 'Stack', [1], 110, 1, 0],\n",
    "    [1, 184, 'DSA', 'Sorting', [1, 0], 85, 0, 0],\n",
    "    [1, 195, 'DSA', 'Queue', [0, 0, 1], 50, 1, 1],\n",
    "    [1, 106, 'DSA', 'Graph', [1], 95, 0, 0],\n",
    "    [1, 117, 'DSA', 'Heap', [1, 0, 1], 60, 1, 0],\n",
    "    [1, 128, 'DSA', 'Tree', [1, 0], 120, 0, 1],\n",
    "    [1, 139, 'DSA', 'Algorithms', [1, 0, 1], 45, 1, 0],\n",
    "    [1, 150, 'DSA', 'Hashing', [1, 0], 75, 0, 1],\n",
    "    [1, 161, 'DSA', 'Linked List', [1, 0, 1], 55, 1, 0],\n",
    "    [1, 172, 'DSA', 'Array', [1], 90, 0, 1],\n",
    "    [1, 183, 'DSA', 'Algorithm', [1, 0, 1], 80, 1, 0],\n",
    "    [1, 201, 'DBMS', 'SQL Basics', [1, 0], 60, 1, 0],\n",
    "    [1, 202, 'DBMS', 'Normalization', [1], 70, 0, 1],\n",
    "    [1, 203, 'DBMS', 'SQL Constraints', [1, 0, 1], 65, 1, 0],\n",
    "    [1, 204, 'DBMS', 'Keys', [1, 0], 80, 0, 1],\n",
    "    [1, 205, 'DBMS', 'SQL Functions', [1], 75, 1, 0],\n",
    "    [1, 206, 'DBMS', 'Joins', [1, 0,1,1], 90, 1, 0],\n",
    "    [1, 207, 'DBMS', 'Stored Procedures', [1, 0, 1], 100, 0, 1],\n",
    "    [1, 208, 'DBMS', 'Transactions', [1], 85, 1, 0],\n",
    "    [1, 209, 'DBMS', 'Database Models', [1, 0,1,1], 95, 0, 1],\n",
    "    [1, 210, 'DBMS', 'Indexing', [1], 110, 1, 0],\n",
    "    [1, 211, 'DBMS', 'Views', [1, 0,1,1], 80, 0, 1],\n",
    "    [1, 212, 'DBMS', 'Database Design', [0], 105, 1, 1],\n",
    "    [1, 213, 'DBMS', 'Security', [1, 0, 1], 90, 0, 0],\n",
    "    [1, 214, 'DBMS', 'Backup & Recovery', [0], 115, 1, 1],\n",
    "    [1, 215, 'DSA', 'Searching', [1, 0,1,1], 45, 0, 1],\n",
    "    [1, 216, 'DSA', 'Stack', [1, 1, 1, 1], 100, 1, 0],\n",
    "    [1, 217, 'DSA', 'Sorting', [0], 80, 0, 1],\n",
    "    [1, 218, 'DSA', 'Queue', [1, 0], 60, 1, 1],\n",
    "    [1, 219, 'DSA', 'Graph', [1, 1, 1, 1], 90, 0, 0],\n",
    "    [1, 220, 'DSA', 'Heap', [0], 70, 1, 0],\n",
    "    [1, 221, 'DSA', 'Tree', [1, 1, 1, 1], 125, 0, 1],\n",
    "    [1, 222, 'DSA', 'Algorithms', [0], 55, 1, 0],\n",
    "    [1, 223, 'DSA', 'Hashing', [1, 0], 85, 0, 1],\n",
    "    [1, 224, 'DSA', 'Linked List', [1, 1, 1, 1], 60, 1, 0],\n",
    "    [1, 225, 'DSA', 'Array', [0], 95, 0, 1],\n",
    "    [1, 226, 'DSA', 'Algorithm', [1, 0], 80, 1, 0],\n",
    "    [1, 227, 'DBMS', 'SQL Basics', [1, 1, 1, 1], 50, 1, 1],\n",
    "    [1, 228, 'DBMS', 'Normalization', [0], 65, 0, 0],\n",
    "    [1, 229, 'DBMS', 'SQL Constraints', [1, 0], 70, 1, 1],\n",
    "    [1, 230, 'DBMS', 'Keys', [1, 0, 1], 85, 0, 1],\n",
    "    [1, 231, 'DBMS', 'SQL Functions', [0], 90, 1, 0],\n",
    "    [1, 232, 'DBMS', 'Joins', [1, 0], 80, 1, 0],\n",
    "    [1, 233, 'DBMS', 'Stored Procedures', [1], 95, 0, 1],\n",
    "    [1, 234, 'DBMS', 'Transactions', [1, 0, 1], 75, 1, 0],\n",
    "    [1, 235, 'DBMS', 'Database Models', [1], 110, 0, 1],\n",
    "    [1, 236, 'DBMS', 'Indexing', [1, 0], 100, 1, 0],\n",
    "    [1, 237, 'DBMS', 'Views', [1], 85, 0, 1],\n",
    "    [1, 238, 'DBMS', 'Database Design', [1, 0], 105, 1, 1],\n",
    "    [1, 239, 'DBMS', 'Security', [1], 95, 0, 0],\n",
    "    [1, 240, 'DBMS', 'Backup & Recovery', [1, 0, 1], 115, 1, 1],\n",
    "    [1, 241, 'DSA', 'Searching', [1], 50, 0, 1],\n",
    "    [1, 242, 'DSA', 'Stack', [1, 0], 120, 1, 0],\n",
    "    [1, 243, 'DSA', 'Sorting', [1, 0, 1], 85, 0, 1],\n",
    "    [1, 244, 'DSA', 'Queue', [1], 65, 1, 1],\n",
    "    [1, 245, 'DSA', 'Graph', [1, 0], 90, 0, 0],\n",
    "    [1, 246, 'DSA', 'Heap', [1, 0, 1], 70, 1, 0],\n",
    "    [1, 247, 'DSA', 'Tree', [1], 125, 0, 1],\n",
    "    [1, 248, 'DSA', 'Algorithms', [1, 0], 55, 1, 0],\n",
    "    [1, 249, 'DSA', 'Hashing', [1], 85, 0, 1],\n",
    "    [1, 250, 'DSA', 'Linked List', [1, 0], 60, 1, 0],\n",
    "    [1, 251, 'DSA', 'Array', [0, 0, 1, 0], 95, 0, 1],\n",
    "    [1, 252, 'DSA', 'Algorithm', [1], 80, 1, 0],\n",
    "    [1, 253, 'DBMS', 'SQL Basics', [1, 0], 55, 1, 1],\n",
    "    [1, 254, 'DBMS', 'Normalization', [1], 75, 0, 1],\n",
    "    [1, 255, 'DBMS', 'SQL Constraints', [0, 0, 1, 0], 70, 1, 0],\n",
    "    [1, 256, 'DBMS', 'Keys', [1], 85, 0, 1],\n",
    "    [1, 257, 'DBMS', 'SQL Functions', [1, 0], 80, 1, 0],\n",
    "    [1, 258, 'DBMS', 'Joins', [1], 90, 1, 0],\n",
    "    [1, 259, 'DBMS', 'Stored Procedures', [1, 0], 100, 0, 1],\n",
    "    [1, 260, 'DBMS', 'Transactions', [1], 65, 1, 0],\n",
    "    [1, 261, 'DBMS', 'Database Models', [0, 0, 1, 0], 95, 0, 1],\n",
    "    [1, 262, 'DBMS', 'Indexing', [1], 105, 1, 0],\n",
    "    [1, 263, 'DBMS', 'Views', [1, 0], 80, 0, 1],\n",
    "    [1, 264, 'DBMS', 'Database Design', [1], 110, 1, 1],\n",
    "    [1, 265, 'DBMS', 'Security', [1, 0], 90, 0, 0],\n",
    "    [1, 266, 'DBMS', 'Backup & Recovery', [1], 115, 1, 1],\n",
    "    [1, 267, 'DSA', 'Searching', [1, 0], 45, 0, 1],\n",
    "    [1, 268, 'DSA', 'Stack', [1], 120, 1, 0]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "df_data_list = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    attCnt = len(data[i][4])\n",
    "    correctness_rate = calculate_correctness_rate(data[i][4])\n",
    "    confidence_level = calculate_confidence_level({\n",
    "        'answer':data[i][6],\n",
    "        'attempt_number':attCnt,\n",
    "        'marked_for_review':data[i][7],\n",
    "        'time_taken':data[i][5]\n",
    "    })\n",
    "    \n",
    "    row = {\n",
    "        'user_id':data[i][0],\n",
    "        'question_id':data[i][1],\n",
    "        'domain':data[i][2],\n",
    "        'subdomain':data[i][3],\n",
    "        'attempt_number':attCnt,\n",
    "        'time_taken':data[i][5],\n",
    "        'answer':data[i][6],\n",
    "        'marked_for_review':data[i][7],\n",
    "        'correctness_rate':correctness_rate,\n",
    "        'confidence_level':confidence_level,\n",
    "    }\n",
    "\n",
    "    df_data_list.append(row)\n",
    "\n",
    "dataFramePrimary = pd.DataFrame(df_data_list)\n",
    "\n",
    "#now to get the data to send to model\n",
    "\n",
    "inpData = dataFramePrimary.drop(columns=['user_id','question_id','domain','subdomain'])\n",
    "\n",
    "# print(inpData)\n",
    "#scale the above data and then make predictions and store them in original df\n",
    "inpDataScale = scaler.transform(inpData)\n",
    "\n",
    "# print(type(inpDataScale))\n",
    "\n",
    "predict=knn.predict(inpDataScale)\n",
    "\n",
    "QuestionLabelledData = []\n",
    "\n",
    "for index,rows in inpData.iterrows():\n",
    "    \n",
    "    nRow = {\n",
    "        'user_id':df_data_list[index].get('user_id'),\n",
    "        'question_id':df_data_list[index].get('question_id'),\n",
    "        'domain':df_data_list[index].get('domain'),\n",
    "        'subdomain':df_data_list[index].get('subdomain'),\n",
    "        'attempt_number':df_data_list[index].get('attempt_number'),\n",
    "        'time_taken':df_data_list[index].get('time_taken'),\n",
    "        'answer':df_data_list[index].get('answer'),\n",
    "        'marked_for_review':df_data_list[index].get('marked_for_review'),\n",
    "        'correctness_rate':df_data_list[index].get('correctness_rate'),\n",
    "        'confidence_level':df_data_list[index].get('confidence_level'),\n",
    "        'label':predict[index]\n",
    "    }\n",
    "    QuestionLabelledData.append(nRow)\n",
    "\n",
    "questionLabeledDataFrame = pd.DataFrame(QuestionLabelledData)\n",
    "\n",
    "#now classifying the topics\n",
    "#selecting the required columns for secondary classification\n",
    "\n",
    "toLabelTopicDataFrame = questionLabeledDataFrame[selected_columns]\n",
    "\n",
    "#calculating percentages for each subdomain\n",
    "\n",
    "tolabelTopic_stats = toLabelTopicDataFrame.groupby('subdomain')['label'].agg(['mean','count']).reset_index()\n",
    "\n",
    "tolabelTopic_stats['percent_repeat'] = tolabelTopic_stats['mean'] * 100\n",
    "print(tolabelTopic_stats)\n",
    "secondth = knn_model.predict(tolabelTopic_stats.drop(columns=['subdomain','mean','count']))\n",
    "\n",
    "topic_classification = []\n",
    "\n",
    "def Tclassify(lab):\n",
    "    if(lab == 0):\n",
    "        return 'Move on from the Topic'\n",
    "    elif(lab==1):\n",
    "        return 'Repeat Same Topic with Less Weightage'\n",
    "    elif(lab==2):\n",
    "        return 'Repeat Same Topic with More Weightage'\n",
    "    else:\n",
    "        return 'Invalid'\n",
    "\n",
    "for i in range(len(secondth)):\n",
    "    newRow = {\n",
    "        'subdomain':tolabelTopic_stats.get('subdomain')[i],\n",
    "        'category':Tclassify(secondth[i]),\n",
    "        'category_encoded':secondth[i]\n",
    "    }\n",
    "    topic_classification.append(newRow)\n",
    "\n",
    "finalClassificationDataFrame = pd.DataFrame(topic_classification)\n",
    "\n",
    "print(finalClassificationDataFrame)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now to generate new question paper\n",
    "\n",
    "#we need to extract the domains and subdomains the question paper needs first\n",
    "#say the question paper has a limit of 20 questions across the domains \n",
    "#so 2 domains, 10 subdomains across them and 2 questions each??\n",
    "#but we also need to increase weightage and decrease weightage\n",
    "#so that means we should increase the number of questions so that we can manage the weightage\n",
    "\n",
    "# so we need to determine how the weightage of domains will affect the paper layout\n",
    "\n",
    "\n",
    "\n",
    "#first deciding the default layout and then according to the weightage we will add the questions \n",
    "#the thing we need to do is if more weightage then increase the amount of difficult questions as well\n",
    "\n",
    "#for the subdomain if the classification is ____ then : \n",
    "#so more weightage : 5 questions\n",
    "#less weightage : 3 questions\n",
    "#move on : 1 question\n",
    "\n",
    "#Generating a default question paper\n",
    "#select the domain\n",
    "#select the subdomains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           subdomain  avg_difficulty  weight  weighted_difficulty  questions\n",
      "0  Backup & Recovery        2.000000       0             0.625000          3\n",
      "1              Joins        2.000000       0             0.625000          3\n",
      "2               Keys        1.714286       0             0.535714          3\n",
      "3      Normalization        2.000000       0             0.625000          3\n",
      "4         SQL Basics        1.481481       0             0.462963          3\n",
      "5    SQL Constraints        1.000000       0             0.312500          3\n",
      "6      SQL Functions        1.346154       0             0.420673          2\n",
      "7           Security        2.000000       0             0.625000          3\n",
      "8       Transactions        2.200000       0             0.687500          3\n",
      "[266, 195, 255, 253, 276, 257, 265, 180, 240, 218, 192, 178, 259, 196, 186, 242, 251, 308, 216, 203, 171, 299]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the question bank dataset\n",
    "QuestionBank = pd.read_csv('questionBank.csv')\n",
    "\n",
    "# List of selected subdomains\n",
    "selected_subdomain = [\n",
    "    'SQL Basics',\n",
    "    'Normalization',\n",
    "    'SQL Constraints',\n",
    "    'Keys',\n",
    "    'SQL Functions',\n",
    "    'Joins',\n",
    "    'Transactions',\n",
    "    'Security',\n",
    "    'Backup & Recovery'\n",
    "]\n",
    "\n",
    "# Mapping the difficulty for each subdomain with a numeric value\n",
    "difficultyMap = {'Easy': 1, 'Medium': 2, 'Hard': 3}\n",
    "QuestionBank['difficulty_numeric'] = QuestionBank['difficulty'].map(difficultyMap)\n",
    "\n",
    "# Filter questions by selected subdomains\n",
    "subdomainQB = QuestionBank[QuestionBank['subdomain'].isin(selected_subdomain)]\n",
    "\n",
    "# Calculate the average difficulty for each subdomain\n",
    "subdomainQB_difficulty = subdomainQB.groupby('subdomain')['difficulty_numeric'].mean().reset_index()\n",
    "subdomainQB_difficulty.columns = ['subdomain', 'avg_difficulty']\n",
    "\n",
    "# Initialize weights dynamically\n",
    "weights = dict.fromkeys(selected_subdomain, 0)\n",
    "\n",
    "# Function to adjust weights\n",
    "def adjust_weights(weights, subdomain, adjustment):\n",
    "    if subdomain in weights:\n",
    "        weights[subdomain] += adjustment\n",
    "        if weights[subdomain] < -1:\n",
    "            weights[subdomain] = -1\n",
    "        elif weights[subdomain] > 1:\n",
    "            weights[subdomain] = 1\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "# Add weight column to DataFrame\n",
    "subdomainQB_difficulty['weight'] = subdomainQB_difficulty['subdomain'].map(weights)\n",
    "\n",
    "# Calculate the number of questions to select from each subdomain\n",
    "min_questions = 1\n",
    "total_questions = random.randint(20, 26)\n",
    "additional_questions = total_questions - min_questions * len(selected_subdomain)\n",
    "\n",
    "# Adjust question calculation to include weights\n",
    "subdomainQB_difficulty['weighted_difficulty'] = (\n",
    "    (subdomainQB_difficulty['avg_difficulty'] + subdomainQB_difficulty['weight']) / \n",
    "    (subdomainQB_difficulty['avg_difficulty'].max() + 1)\n",
    ")\n",
    "\n",
    "subdomainQB_difficulty['questions'] = min_questions + (\n",
    "    (subdomainQB_difficulty['weighted_difficulty'] / subdomainQB_difficulty['weighted_difficulty'].sum()) * additional_questions\n",
    ").astype(int)\n",
    "\n",
    "# Adjust to ensure the total number of questions is within the required range\n",
    "if subdomainQB_difficulty['questions'].sum() > total_questions:\n",
    "    while subdomainQB_difficulty['questions'].sum() > total_questions:\n",
    "        max_index = subdomainQB_difficulty['questions'].idxmax()\n",
    "        subdomainQB_difficulty.at[max_index, 'questions'] -= 1\n",
    "elif subdomainQB_difficulty['questions'].sum() < total_questions:\n",
    "    while subdomainQB_difficulty['questions'].sum() < total_questions:\n",
    "        min_index = subdomainQB_difficulty['questions'].idxmin()\n",
    "        subdomainQB_difficulty.at[min_index, 'questions'] += 1\n",
    "print(subdomainQB_difficulty)\n",
    "# Ensure questions include all three difficulties and prioritize hard questions\n",
    "selected_QIDS = []\n",
    "for _, row in subdomainQB_difficulty.iterrows():\n",
    "    subdomain = row['subdomain']\n",
    "    num_questions = row['questions']\n",
    "\n",
    "    subdomain_questions = subdomainQB[subdomainQB['subdomain'] == subdomain]\n",
    "\n",
    "    easy_questions = subdomain_questions[subdomain_questions['difficulty'] == 'Easy']\n",
    "    medium_questions = subdomain_questions[subdomain_questions['difficulty'] == 'Medium']\n",
    "    hard_questions = subdomain_questions[subdomain_questions['difficulty'] == 'Hard']\n",
    "\n",
    "    num_hard = min(num_questions // 2, len(hard_questions))\n",
    "    num_easy_medium = num_questions - num_hard\n",
    "\n",
    "    num_easy = min(num_easy_medium // 2, len(easy_questions))\n",
    "    num_medium = num_easy_medium - num_easy\n",
    "\n",
    "    if num_easy + num_medium + num_hard < num_questions:\n",
    "        remaining = num_questions - (num_easy + num_medium + num_hard)\n",
    "        num_hard += remaining\n",
    "\n",
    "    # Now randomly selecting questions, ensuring they don't repeat frequently\n",
    "    selected_QIDS += random.sample(list(hard_questions['qID']), min(num_hard, len(hard_questions)))\n",
    "    selected_QIDS += random.sample(list(medium_questions['qID']), min(num_medium, len(medium_questions)))\n",
    "    selected_QIDS += random.sample(list(easy_questions['qID']), min(num_easy, len(easy_questions)))        \n",
    "\n",
    "# Adjust the number of selected questions to ensure it falls within the required range\n",
    "while len(selected_QIDS) < 20:\n",
    "    for _, row in subdomainQB_difficulty.iterrows():\n",
    "        subdomain = row['subdomain']\n",
    "        subdomain_questions = subdomainQB[subdomainQB['subdomain'] == subdomain]\n",
    "        remaining_questions = subdomain_questions[~subdomain_questions['qID'].isin(selected_QIDS)]\n",
    "        if len(remaining_questions) > 0:\n",
    "            selected_QIDS.append(remaining_questions.sample(1)['qID'].values[0])\n",
    "        if len(selected_QIDS) >= 20:\n",
    "            break\n",
    "\n",
    "while len(selected_QIDS) > 26:\n",
    "    selected_QIDS.pop()\n",
    "\n",
    "# Shuffle the selected qIDs to ensure a different order each run\n",
    "random.shuffle(selected_QIDS)\n",
    "\n",
    "print(selected_QIDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Himitsu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
